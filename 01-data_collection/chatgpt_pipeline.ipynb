{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import statistics\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for abnormal and 0 for normal\n",
    "ABNORMAL = 1\n",
    "NORMAL = 0\n",
    "\n",
    "class base_CFG:\n",
    "    data_path = 'data/bgl/bgl_structured_unique.csv'\n",
    "    tmp_fold = 'cache/'\n",
    "\n",
    "    ##########################################\n",
    "    # openai.api setting\n",
    "    ##########################################\n",
    "    openai.organization = 'YOUR-ORG-ID'\n",
    "    openai.api_key = 'YOUR-API-KEY'\n",
    "\n",
    "    ##########################################\n",
    "    # openai model parameters setting\n",
    "    ##########################################\n",
    "    openai_model = \"gpt-3.5-turbo\"\n",
    "    temperature = 0.3  # between 0 and 2\n",
    "    response_num = 5   # How many chat completion choices to generate for each input message.\n",
    "    frequency_penalty = 0.5\n",
    "\n",
    "    pos_num = 0\n",
    "    neg_num = 0\n",
    "\n",
    "    user_prompt = 'explain_log'\n",
    "\n",
    "    ##########################################\n",
    "    # exponential fallback retry setting\n",
    "    ##########################################\n",
    "    max_retries = 10\n",
    "    exponential_base = 2\n",
    "    jitter = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_data(cfg):\n",
    "    df = pd.read_csv(cfg.data_path)\n",
    "    df['target'] = df['label_info'].apply(lambda x : NORMAL if x == '-' else ABNORMAL)\n",
    "    df['full_log'] = df['full_log'].str.lower()\n",
    "    df['log_message'] = df.full_log.apply(lambda x : str(' '.join(x.split(' ')[5:])).lower())\n",
    "    \n",
    "\n",
    "    print(f\"data shape: {df.shape}, data columns: {df.columns}\")\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegenerateError(Exception):\n",
    "    \"\"\"\n",
    "    Exception: \n",
    "        incomplete response need to be regenerated.\n",
    "    \"\"\"\n",
    "    def __init__(self, regenerate_cnt, *args: object) -> None:\n",
    "        super().__init__(*args)\n",
    "        self.regenerate_cnt = regenerate_cnt\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Error: {self.regenerate_cnt} responses need to be regenerated.'\n",
    "    \n",
    "class MaxRegenerateError(Exception):\n",
    "    \"\"\"\n",
    "    Exception: maximum number of regenerations reached.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: object) -> None:\n",
    "        super().__init__(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_one_log(system_prompt, user_prompt, model='gpt-3.5-turbo', \n",
    "                       temperature=0.5, response_num=1, frequency_penalty=0):\n",
    "    \"\"\"\n",
    "    call openai.ChatCompletion and return completion.choices\n",
    "    \"\"\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature = temperature,\n",
    "        n = response_num,\n",
    "        frequency_penalty = frequency_penalty\n",
    "    )\n",
    "\n",
    "    \"\"\"Chat Completion Response\n",
    "    {\n",
    "        \"choices\": [\n",
    "        {\n",
    "            \"finish_reason\": \"stop\", # or 'length' or 'function_call' or 'content_filter'\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n",
    "                \"role\": \"assistant\"\n",
    "            }\n",
    "        }],\n",
    "        \"created\": 1677664795,\n",
    "        \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
    "        \"model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"usage\": {\n",
    "            \"completion_tokens\": 17,\n",
    "            \"prompt_tokens\": 57,\n",
    "            \"total_tokens\": 74\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    return completion.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_one_log_with_retry(system_prompt, user_prompt, cfg:base_CFG):\n",
    "    num_retries = 0\n",
    "    delay_seconds = 10\n",
    "    regenerate_retries = 0\n",
    "\n",
    "    res = []\n",
    "    response_num_cache = cfg.response_num\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choices = completion_one_log(system_prompt, user_prompt, model=cfg.openai_model,\n",
    "                                         temperature=cfg.temperature, response_num=response_num_cache,\n",
    "                                         frequency_penalty=cfg.frequency_penalty)\n",
    "            \n",
    "            ##############################\n",
    "            # Check the generated text. Regenerate if necessary.\n",
    "            ##############################\n",
    "            regenerate_cnt = 0\n",
    "            for choice in choices:\n",
    "                # generated text is truncated\n",
    "                if choice['finish_reason'] == 'length':\n",
    "                    print(f'trunc : ' + choice['message']['content'])\n",
    "                    regenerate_cnt += 1\n",
    "                    continue\n",
    "                # save response\n",
    "                res.append(choice['message']['content'])\n",
    "\n",
    "            if regenerate_cnt:\n",
    "                raise RegenerateError(regenerate_cnt)\n",
    "            else:\n",
    "                return res\n",
    "        \n",
    "        except RegenerateError as e:\n",
    "            response_num_cache = e.regenerate_cnt\n",
    "\n",
    "            regenerate_retries += 1\n",
    "            if regenerate_retries > cfg.max_retries:\n",
    "                raise MaxRegenerateError()\n",
    "            time.sleep(delay_seconds)\n",
    "        \n",
    "        except Exception as e:\n",
    "            num_retries += 1\n",
    "            if num_retries > cfg.max_retries:\n",
    "                raise Exception(\n",
    "                        f\"Maximum number of retries ({cfg.max_retries}) exceeded.\"\n",
    "                    )\n",
    "            \n",
    "            print(f\"{e}, retry No.{num_retries}, delay {delay_seconds}s\")\n",
    "            \n",
    "            # Increment the delay\n",
    "            delay_seconds *= cfg.exponential_base * (1 + cfg.jitter * random.random())\n",
    "            time.sleep(delay_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt(log_df, log_row, cfg):\n",
    "    # get few shot\n",
    "    few_shot_prompt = \"\"\n",
    "    if cfg.pos_num != 0:\n",
    "        few_shot_prompt += '\\n'.join(log_df[log_df.target == NORMAL].head(cfg.pos_num).full_log.apply(lambda x: '- \"' + x + '\" is a normal log').values)\n",
    "        few_shot_prompt += '\\n'\n",
    "    if cfg.neg_num != 0:\n",
    "        few_shot_prompt += '\\n'.join(log_df[log_df.target == ABNORMAL].head(cfg.neg_num).full_log.apply(lambda x: '- \"' + x + '\" is an abnormal log').values)\n",
    "        few_shot_prompt += '\\n'\n",
    "    if few_shot_prompt != \"\":\n",
    "        few_shot_prompt = \"examples:\\n\" + few_shot_prompt\n",
    "\n",
    "    # full log as prompt\n",
    "    if cfg.user_prompt == 'full_log':\n",
    "        return few_shot_prompt + \"question:\\n\" + log_row['full_log']\n",
    "    \n",
    "    # ask for reason with a log and its label\n",
    "    if cfg.user_prompt == 'ask_reason':\n",
    "        full_log, label = log_row['full_log'], log_row['target']\n",
    "        label = 'normal' if label == NORMAL else 'abnormal'\n",
    "        prompt = few_shot_prompt + f'why does log \"{full_log}\" is labeled as {label}?'\n",
    "        if label == 'abnormal':\n",
    "            prompt += ' how to solve this abnormal log?'\n",
    "        return prompt\n",
    "\n",
    "    # ChatGPT expand and explain log. Log message include log level.\n",
    "    if cfg.user_prompt == 'explain_log':\n",
    "        log_message = log_row['log_message']\n",
    "        prompt = f'expand and explain this log : \"{log_message}\"'\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_logs(log_df, cfg, past_res=None):\n",
    "    # restore past result\n",
    "    if past_res == None:\n",
    "        res = []\n",
    "    else:\n",
    "        res = past_res \n",
    "\n",
    "    # requests\n",
    "    for index, row in tqdm(log_df.iterrows(), total = len(log_df)):\n",
    "        if index < len(res):\n",
    "            continue\n",
    "        try:\n",
    "            user_prompt = get_user_prompt(log_df, row, cfg)\n",
    "            res.append(request_one_log_with_retry(cfg.system_prompt, user_prompt, cfg))\n",
    "        except MaxRegenerateError as e:\n",
    "            print(e)\n",
    "            res.append(['MaxRegenerateError'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"save temp result to {cfg.temp_fold}response.csv, successful length is {len(res)}.\")\n",
    "            pd.DataFrame(res).to_csv(f'{cfg.temp_fold}response.csv', index=False, header=False)\n",
    "            res.append(['OtherError'])\n",
    "        time.sleep(10)\n",
    "            \n",
    "    log_df['responses'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_res(cfg):\n",
    "    res_df = pd.read_csv(f'{cfg.temp_fold}response.csv', header=None)\n",
    "    res = []\n",
    "    for _, row in res_df.iterrows():\n",
    "        res.append(list(row))\n",
    "    del res_df\n",
    "    gc.collect()\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG1(base_CFG):\n",
    "    exp_name = 'June_exp1_explain_and_expand'\n",
    "    system_prompt = 'You are a log analysis expert. Most of the raw logs are concise, so based on your extensive experience in log analysis, you should expand and explain the original logs. The better the information you provide, the more helpful it will be for downstream log anomaly detection tasks and operations engineers. To provide better information, you need to focus on the following aspects:\\n\\n1. Convert abbreviations to full words or use parentheses to explain them.\\n2. Convert unatural string to natual language. For example convert \"2005-06-03-15.42.50.363779\" into \"timestamp(2005-06-03-15.42.50.363779)\"\\n3. Pay attention to the content after the last colon of log message. Explain what is it and why does it happen and what will it cause potentially.\\n4. Pay attention to the parameter part and the number part. For example, exit code, indicating 0 is normal, otherwise it is an exception. 0 and 1 may represent down and up in command. For file path parameters, explain the meaning of every sub-path. For other strings, explaining their meaning.\\n5. For very short logs without colons nor parameters, focus on providing as much relevant information as possible related to that particular log entry.\\n6. Do not summarize'\n",
    "\n",
    "CFG_list = [\n",
    "    CFG1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (302, 7), data columns: Index(['line_id', 'label_info', 'full_log', 'log_message', 'template',\n",
      "       'target', 'weight'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>label_info</th>\n",
       "      <th>full_log</th>\n",
       "      <th>log_message</th>\n",
       "      <th>template</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838570 2005.06.03 r02-m1-n0-c:j12-u11 2005...</td>\n",
       "      <td>ras kernel info instruction cache parity error...</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1477</td>\n",
       "      <td>-</td>\n",
       "      <td>1117838840 2005.06.03 r27-m1-l3-u18-c 2005-06-...</td>\n",
       "      <td>ras linkcard info midplaneswitchcontroller per...</td>\n",
       "      <td>MidplaneSwitchController performing bit sparin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2735</td>\n",
       "      <td>-</td>\n",
       "      <td>1117839085 2005.06.03 r20-m1-n5-c:j17-u01 2005...</td>\n",
       "      <td>ras kernel info generating core.304</td>\n",
       "      <td>generating (.*)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4882</td>\n",
       "      <td>-</td>\n",
       "      <td>1117839710 2005.06.03 r16-m1-n2-c:j17-u01 2005...</td>\n",
       "      <td>ras kernel info 1 ddr errors(s) detected and c...</td>\n",
       "      <td>(.*) ddr errorss detected and corrected on ran...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4883</td>\n",
       "      <td>-</td>\n",
       "      <td>1117839710 2005.06.03 r13-m0-na-c:j14-u01 2005...</td>\n",
       "      <td>ras kernel info 3450051 l3 edram error(s) (dcr...</td>\n",
       "      <td>(.*) L3 (.*) errors dcr (.*) detected and corr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_id label_info                                           full_log  \\\n",
       "0        1          -  1117838570 2005.06.03 r02-m1-n0-c:j12-u11 2005...   \n",
       "1     1477          -  1117838840 2005.06.03 r27-m1-l3-u18-c 2005-06-...   \n",
       "2     2735          -  1117839085 2005.06.03 r20-m1-n5-c:j17-u01 2005...   \n",
       "3     4882          -  1117839710 2005.06.03 r16-m1-n2-c:j17-u01 2005...   \n",
       "4     4883          -  1117839710 2005.06.03 r13-m0-na-c:j14-u01 2005...   \n",
       "\n",
       "                                         log_message  \\\n",
       "0  ras kernel info instruction cache parity error...   \n",
       "1  ras linkcard info midplaneswitchcontroller per...   \n",
       "2                ras kernel info generating core.304   \n",
       "3  ras kernel info 1 ddr errors(s) detected and c...   \n",
       "4  ras kernel info 3450051 l3 edram error(s) (dcr...   \n",
       "\n",
       "                                            template  target    weight  \n",
       "0           instruction cache parity error corrected       0  0.022309  \n",
       "1  MidplaneSwitchController performing bit sparin...       0  0.000203  \n",
       "2                                    generating (.*)       0  0.359470  \n",
       "3  (.*) ddr errorss detected and corrected on ran...       0  0.007148  \n",
       "4  (.*) L3 (.*) errors dcr (.*) detected and corr...       0  0.001048  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_and_preprocess_data(base_CFG)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'expand and explain this log : \"ras kernel info instruction cache parity error corrected\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_prompt(df, df.loc[0], CFG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "for index, cfg in enumerate(CFG_list):\n",
    "    request_logs(df, cfg, past_res=None)\n",
    "    df.to_csv(f'data/chatgpt_explain_data/{cfg.exp_name}_NO.{index+1}.csv', index=False)\n",
    "    # df.to_csv(f'out/{cfg.exp_name}_temp{cfg.temperature}_n{cfg.response_num}_freq{cfg.frequency_penalty}_pos{cfg.pos_num}_neg{cfg.neg_num}.csv', index=None)\n",
    "    df.drop('responses', axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af5a88a29c9657f0761de652cb405995f678c86c46afeea753dc663c103ad1ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
